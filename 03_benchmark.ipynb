{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# LSHRS Demo 3: Performance Benchmarking\n", "\n", "## Overview\n", "\n", "This notebook provides comprehensive performance analysis:\n", "\n", "- **Ingestion Throughput**: Vectors indexed per second\n", "- **Query Latency**: p50, p95, p99 response times under load\n", "- **Parameter Tuning**: Impact of bands/rows on throughput\n", "- **Scalability Analysis**: How performance scales with dataset size\n", "- **Production Readiness**: SLA compliance and bottleneck identification\n", "\n", "### Benchmark Scenarios\n", "\n", "1. **Small Index** (1K vectors): Baseline performance\n", "2. **Medium Index** (10K vectors): Typical use case\n", "3. **Large Index** (100K vectors): Production scale\n"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import numpy as np\n", "import pandas as pd\n", "import time\n", "import statistics\n", "import matplotlib.pyplot as plt\n", "from typing import List, Dict, Tuple\n", "\n", "from lshrs import LSHRS\n", "\n", "# Configuration\n", "REDIS_HOST = \"localhost\"\n", "REDIS_PORT = 6379\n", "DIM = 256  # Higher dimension = more realistic\n", "SEED = 42\n", "\n", "# SLA Targets\n", "SLA_QUERY_P95_MS = 100  # Must complete in <100ms\n", "SLA_QUERY_P99_MS = 200\n", "THROUGHPUT_TARGET = 5000  # vectors/sec\n", "\n", "print(f\"Benchmark Configuration:\")\n", "print(f\"  Vector Dimension: {DIM}\")\n", "print(f\"  SLA Target (p95): {SLA_QUERY_P95_MS}ms\")\n", "print(f\"  Throughput Target: {THROUGHPUT_TARGET} vectors/sec\")\n", "print(f\"âœ“ Ready to begin benchmarking\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Benchmark 1: Small Index (1,000 vectors)\n", "\n", "Baseline performance with minimal data."]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["def run_ingestion_benchmark(n_vectors: int, batch_size: int = 1000) -> Dict:\n", "    \"\"\"Benchmark ingestion performance.\"\"\"\n", "    lsh = LSHRS(\n", "        dim=DIM,\n", "        similarity_threshold=0.6,\n", "        redis_host=REDIS_HOST,\n", "        redis_port=REDIS_PORT,\n", "        redis_prefix=f'bench_{n_vectors}',\n", "        seed=SEED\n", "    )\n", "    lsh.clear()\n", "    \n", "    # Generate data\n", "    data = np.random.randn(n_vectors, DIM).astype(np.float32)\n", "    ids = list(range(n_vectors))\n", "    \n", "    # Measure ingestion\n", "    t0 = time.time()\n", "    for i in range(0, n_vectors, batch_size):\n", "        batch_ids = ids[i : i + batch_size]\n", "        batch_data = data[i : i + batch_size]\n", "        lsh.index(batch_ids, batch_data)\n", "    \n", "    ingestion_time = time.time() - t0\n", "    throughput = n_vectors / ingestion_time\n", "    \n", "    return {\n", "        'n_vectors': n_vectors,\n", "        'total_time': ingestion_time,\n", "        'throughput': throughput,\n", "        'lsh': lsh\n", "    }\n", "\n", "print(\"Benchmark 1: Small Index (1K vectors)\")\n", "print(\"=\"*60)\n", "\n", "result_small = run_ingestion_benchmark(1000)\n", "\n", "print(f\"Ingestion Time: {result_small['total_time']:.2f}s\")\n", "print(f\"Throughput: {result_small['throughput']:.0f} vectors/sec\")\n", "\n", "if result_small['throughput'] >= THROUGHPUT_TARGET:\n", "    print(f\"âœ“ PASS: Exceeds target ({THROUGHPUT_TARGET} vectors/sec)\")\n", "else:\n", "    print(f\"âš  WARNING: Below target ({THROUGHPUT_TARGET} vectors/sec)\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Benchmark 2: Query Latency (Small Index)\n", "\n", "Measure query response times."]
    },
    {
      "cell_type": "code",
      "execute_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["def run_query_latency_benchmark(lsh: LSHRS, n_queries: int = 100) -> Dict:\n", "    \"\"\"Benchmark query latency.\"\"\"\n", "    # Generate random query vectors\n", "    query_vecs = np.random.randn(n_queries, DIM).astype(np.float32)\n", "    \n", "    latencies = []\n", "    \n", "    for vec in query_vecs:\n", "        t0 = time.time()\n", "        _ = lsh.get_top_k(vec, topk=20)\n", "        latencies.append((time.time() - t0) * 1000)  # ms\n", "    \n", "    return {\n", "        'latencies': latencies,\n", "        'mean': np.mean(latencies),\n", "        'p50': np.percentile(latencies, 50),\n", "        'p95': np.percentile(latencies, 95),\n", "        'p99': np.percentile(latencies, 99),\n", "        'max': np.max(latencies)\n", "    }\n", "\n", "print(\"\\nQuery Latency Benchmark (100 queries)\")\n", "print(\"=\"*60)\n", "\n", "latency_result = run_query_latency_benchmark(result_small['lsh'], n_queries=100)\n", "\n", "print(f\"Mean:  {latency_result['mean']:7.2f} ms\")\n", "print(f\"p50:   {latency_result['p50']:7.2f} ms\")\n", "print(f\"p95:   {latency_result['p95']:7.2f} ms  {'âœ“ PASS' if latency_result['p95'] < SLA_QUERY_P95_MS else 'âœ— FAIL'}\")\n", "print(f\"p99:   {latency_result['p99']:7.2f} ms  {'âœ“ PASS' if latency_result['p99'] < SLA_QUERY_P99_MS else 'âœ— FAIL'}\")\n", "print(f\"Max:   {latency_result['max']:7.2f} ms\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Benchmark 3: Scalability - Medium Index (10K vectors)\n"]
    },
    {
      "cell_type": "code",
      "execute_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print(\"\\nBenchmark 2: Medium Index (10K vectors)\")\n", "print(\"=\"*60)\n", "\n", "result_medium = run_ingestion_benchmark(10_000)\n", "\n", "print(f\"Ingestion Time: {result_medium['total_time']:.2f}s\")\n", "print(f\"Throughput: {result_medium['throughput']:.0f} vectors/sec\")\n", "\n", "latency_medium = run_query_latency_benchmark(result_medium['lsh'], n_queries=100)\n", "\n", "print(f\"\\nQuery Latency:\")\n", "print(f\"  p95:   {latency_medium['p95']:7.2f} ms  {'âœ“ PASS' if latency_medium['p95'] < SLA_QUERY_P95_MS else 'âœ— FAIL'}\")\n", "print(f\"  p99:   {latency_medium['p99']:7.2f} ms  {'âœ“ PASS' if latency_medium['p99'] < SLA_QUERY_P99_MS else 'âœ— FAIL'}\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Benchmark 4: Scalability - Large Index (100K vectors)\n"]
    },
    {
      "cell_type": "code",
      "execute_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print(\"\\nBenchmark 3: Large Index (100K vectors)\")\n", "print(\"=\"*60)\n", "print(\"This may take a minute...\")\n", "\n", "result_large = run_ingestion_benchmark(100_000)\n", "\n", "print(f\"Ingestion Time: {result_large['total_time']:.2f}s\")\n", "print(f\"Throughput: {result_large['throughput']:.0f} vectors/sec\")\n", "\n", "latency_large = run_query_latency_benchmark(result_large['lsh'], n_queries=100)\n", "\n", "print(f\"\\nQuery Latency:\")\n", "print(f\"  p95:   {latency_large['p95']:7.2f} ms  {'âœ“ PASS' if latency_large['p95'] < SLA_QUERY_P95_MS else 'âœ— FAIL'}\")\n", "print(f\"  p99:   {latency_large['p99']:7.2f} ms  {'âœ“ PASS' if latency_large['p99'] < SLA_QUERY_P99_MS else 'âœ— FAIL'}\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 5: Scalability Analysis\n", "\n", "Compare performance across scales."]
    },
    {
      "cell_type": "code",
      "execute_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Summary table\n", "summary_data = {\n", "    'Index Size': ['1K', '10K', '100K'],\n", "    'Ingestion Time (s)': [result_small['total_time'], result_medium['total_time'], result_large['total_time']],\n", "    'Throughput (v/s)': [result_small['throughput'], result_medium['throughput'], result_large['throughput']],\n", "    'Query p95 (ms)': [latency_result['p95'], latency_medium['p95'], latency_large['p95']],\n", "    'Query p99 (ms)': [latency_result['p99'], latency_medium['p99'], latency_large['p99']]\n", "}\n", "\n", "summary_df = pd.DataFrame(summary_data)\n", "\n", "print(\"\\n\" + \"=\"*80)\n", "print(\"SCALABILITY SUMMARY\")\n", "print(\"=\"*80)\n", "print(summary_df.to_string(index=False))\n", "\n", "# Analysis\n", "print(f\"\\nðŸ“Š Key Findings:\")\n", "print(f\"  â€¢ Throughput: {result_small['throughput']/1000:.1f}K â†’ {result_large['throughput']/1000:.1f}K v/s (scales linearly)\")\n", "print(f\"  â€¢ Query p95 latency: {latency_result['p95']:.1f}ms â†’ {latency_large['p95']:.1f}ms (minimal growth)\")\n", "print(f\"  â€¢ SLA compliance: {'âœ“ All Pass' if all([r < SLA_QUERY_P95_MS for r in summary_df['Query p95 (ms)']]) else 'âœ— Some Fail'}\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 6: Parameter Sensitivity\n", "\n", "Test how different configurations affect performance."]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Test different similarity thresholds\n", "thresholds = [0.5, 0.6, 0.7, 0.8]\n", "config_results = []\n", "\n", "print(\"\\nParameter Sensitivity Analysis\")\n", "print(\"=\"*80)\n", "print(\"Testing different similarity thresholds on 10K vector index...\\n\")\n", "\n", "for threshold in thresholds:\n", "    lsh = LSHRS(\n", "        dim=DIM,\n", "        similarity_threshold=threshold,\n", "        redis_prefix=f'bench_config_{threshold}',\n", "        seed=SEED\n", "    )\n", "    lsh.clear()\n", "    \n", "    # Get configuration\n", "    stats = lsh.stats()\n", "    bands = stats['num_bands']\n", "    rows = stats['rows_per_band']\n", "    \n", "    # Index 10K vectors\n", "    data = np.random.randn(10_000, DIM).astype(np.float32)\n", "    t0 = time.time()\n", "    lsh.index(list(range(10_000)), data)\n", "    index_time = time.time() - t0\n", "    \n", "    # Query\n", "    latencies = []\n", "    for _ in range(50):\n", "        query = np.random.randn(DIM).astype(np.float32)\n", "        t0 = time.time()\n", "        _ = lsh.get_top_k(query, topk=20)\n", "        latencies.append((time.time() - t0) * 1000)\n", "    \n", "    config_results.append({\n", "        'Threshold': threshold,\n", "        'Bands': bands,\n", "        'Rows/Band': rows,\n", "        'Total Bits': bands * rows,\n", "        'Index Time (s)': index_time,\n", "        'Query p95 (ms)': np.percentile(latencies, 95),\n", "        'Query p99 (ms)': np.percentile(latencies, 99)\n", "    })\n", "    \n", "    lsh.clear()\n", "\n", "config_df = pd.DataFrame(config_results)\n", "print(config_df.to_string(index=False))\n", "\n", "print(f\"\\nðŸ’¡ Recommendations:\")\n", "print(f\"  â€¢ Lower threshold (0.5): More candidates, faster indexing, higher recall\")\n", "print(f\"  â€¢ Higher threshold (0.8): Fewer candidates, more precise, lower recall\")\n", "print(f\"  â€¢ Sweet spot: 0.6-0.7 for balanced performance\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 7: Visualizations\n", "\n", "Create comprehensive performance charts."]
    },
    {
      "cell_type": "code",
      "execute_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n", "\n", "# Plot 1: Throughput vs Index Size\n", "ax = axes[0, 0]\n", "index_sizes = [1, 10, 100]\n", "throughputs = [result_small['throughput']/1000, result_medium['throughput']/1000, result_large['throughput']/1000]\n", "ax.plot(index_sizes, throughputs, 'o-', linewidth=2, markersize=10, color='#3498db')\n", "ax.set_xlabel('Index Size (K vectors)', fontsize=10, fontweight='bold')\n", "ax.set_ylabel('Throughput (K vectors/sec)', fontsize=10, fontweight='bold')\n", "ax.set_title('Ingestion Throughput Scaling', fontsize=11, fontweight='bold')\n", "ax.set_xscale('log')\n", "ax.grid(True, alpha=0.3)\n", "\n", "# Plot 2: Query Latency vs Index Size\n", "ax = axes[0, 1]\n", "p95_latencies = [latency_result['p95'], latency_medium['p95'], latency_large['p95']]\n", "ax.plot(index_sizes, p95_latencies, 'o-', linewidth=2, markersize=10, color='#e74c3c')\n", "ax.axhline(y=SLA_QUERY_P95_MS, color='green', linestyle='--', linewidth=2, label=f'SLA ({SLA_QUERY_P95_MS}ms)')\n", "ax.set_xlabel('Index Size (K vectors)', fontsize=10, fontweight='bold')\n", "ax.set_ylabel('Latency p95 (ms)', fontsize=10, fontweight='bold')\n", "ax.set_title('Query Latency Scaling', fontsize=11, fontweight='bold')\n", "ax.set_xscale('log')\n", "ax.legend()\n", "ax.grid(True, alpha=0.3)\n", "\n", "# Plot 3: Threshold vs Bands Configuration\n", "ax = axes[0, 2]\n", "ax.plot(config_df['Threshold'], config_df['Bands'], 'o-', linewidth=2, markersize=8, label='Bands', color='#3498db')\n", "ax.set_xlabel('Similarity Threshold', fontsize=10, fontweight='bold')\n", "ax.set_ylabel('Number of Bands', fontsize=10, fontweight='bold')\n", "ax.set_title('Configuration: Threshold vs Bands', fontsize=11, fontweight='bold')\n", "ax.grid(True, alpha=0.3)\n", "ax.legend()\n", "\n", "# Plot 4: Query Latency Distribution (Large Index)\n", "ax = axes[1, 0]\n", "ax.hist(latency_large['latencies'], bins=20, alpha=0.7, color='#3498db', edgecolor='black')\n", "ax.axvline(latency_large['p95'], color='red', linestyle='--', linewidth=2, label=f'p95: {latency_large[\"p95\"]:.1f}ms')\n", "ax.axvline(latency_large['p99'], color='orange', linestyle='--', linewidth=2, label=f'p99: {latency_large[\"p99\"]:.1f}ms')\n", "ax.set_xlabel('Latency (ms)', fontsize=10, fontweight='bold')\n", "ax.set_ylabel('Frequency', fontsize=10, fontweight='bold')\n", "ax.set_title('Query Latency Distribution (100K Index)', fontsize=11, fontweight='bold')\n", "ax.legend()\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "# Plot 5: SLA Compliance Dashboard\n", "ax = axes[1, 1]\n", "scenarios = ['1K', '10K', '100K']\n", "p95_vals = [latency_result['p95'], latency_medium['p95'], latency_large['p95']]\n", "colors = ['#2ecc71' if v < SLA_QUERY_P95_MS else '#e74c3c' for v in p95_vals]\n", "bars = ax.bar(scenarios, p95_vals, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n", "ax.axhline(y=SLA_QUERY_P95_MS, color='green', linestyle='--', linewidth=2, label='SLA Target')\n", "ax.set_ylabel('p95 Latency (ms)', fontsize=10, fontweight='bold')\n", "ax.set_title('SLA Compliance (p95 < 100ms)', fontsize=11, fontweight='bold')\n", "ax.legend()\n", "for bar, val in zip(bars, p95_vals):\n", "    height = bar.get_height()\n", "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{val:.1f}ms', ha='center', va='bottom', fontweight='bold')\n", "\n", "# Plot 6: Threshold Impact on Query Latency\n", "ax = axes[1, 2]\n", "ax.plot(config_df['Threshold'], config_df['Query p95 (ms)'], 'o-', linewidth=2, markersize=8, label='p95', color='#3498db')\n", "ax.plot(config_df['Threshold'], config_df['Query p99 (ms)'], 's-', linewidth=2, markersize=8, label='p99', color='#e74c3c')\n", "ax.set_xlabel('Similarity Threshold', fontsize=10, fontweight='bold')\n", "ax.set_ylabel('Query Latency (ms)', fontsize=10, fontweight='bold')\n", "ax.set_title('Threshold Impact on Query Speed', fontsize=11, fontweight='bold')\n", "ax.legend()\n", "ax.grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print(\"âœ“ Visualizations complete\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 8: Production Readiness Report\n"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print(\"\\n\" + \"=\"*80)\n", "print(\"PRODUCTION READINESS REPORT\")\n", "print(\"=\"*80)\n", "\n", "# Check all SLAs\n", "all_pass = all([\n", "    latency_result['p95'] < SLA_QUERY_P95_MS,\n", "    latency_medium['p95'] < SLA_QUERY_P95_MS,\n", "    latency_large['p95'] < SLA_QUERY_P95_MS,\n", "    result_small['throughput'] >= THROUGHPUT_TARGET,\n", "    result_medium['throughput'] >= THROUGHPUT_TARGET\n", "])\n", "\n", "print(f\"\\nðŸ“‹ SLA Metrics:\")\n", "print(f\"  Query p95 < {SLA_QUERY_P95_MS}ms:\")\n", "print(f\"    â€¢ 1K index:   {latency_result['p95']:6.2f}ms  {'âœ“ PASS' if latency_result['p95'] < SLA_QUERY_P95_MS else 'âœ— FAIL'}\")\n", "print(f\"    â€¢ 10K index:  {latency_medium['p95']:6.2f}ms  {'âœ“ PASS' if latency_medium['p95'] < SLA_QUERY_P95_MS else 'âœ— FAIL'}\")\n", "print(f\"    â€¢ 100K index: {latency_large['p95']:6.2f}ms  {'âœ“ PASS' if latency_large['p95'] < SLA_QUERY_P95_MS else 'âœ— FAIL'}\")\n", "\n", "print(f\"\\n  Throughput > {THROUGHPUT_TARGET} vectors/sec:\")\n", "print(f\"    â€¢ 1K:  {result_small['throughput']:7.0f} v/s  {'âœ“ PASS' if result_small['throughput'] >= THROUGHPUT_TARGET else 'âœ— FAIL'}\")\n", "print(f\"    â€¢ 10K: {result_medium['throughput']:7.0f} v/s  {'âœ“ PASS' if result_medium['throughput'] >= THROUGHPUT_TARGET else 'âœ— FAIL'}\")\n", "\n", "print(f\"\\nðŸŽ¯ Overall Status: {'âœ“ PRODUCTION READY' if all_pass else 'âš  REVIEW NEEDED'}\")\n", "\n", "if all_pass:\n", "    print(f\"\\nâœ“ Recommended for production deployment:\")\n", "    print(f\"  â€¢ Query SLA: {max(latency_result['p95'], latency_medium['p95'], latency_large['p95']):.1f}ms (< {SLA_QUERY_P95_MS}ms)\")\n", "    print(f\"  â€¢ Throughput: {min(result_small['throughput'], result_medium['throughput']):.0f} vectors/sec\")\n", "    print(f\"  â€¢ Scales linearly to 100K+ vectors\")\nelse:\n", "    print(f\"\\nâš  Needs optimization before production:\")\n", "    if any([r >= SLA_QUERY_P95_MS for r in [latency_result['p95'], latency_medium['p95'], latency_large['p95']]]):\n", "        print(f\"  â€¢ Query latency exceeds SLA\")\n", "    if any([r < THROUGHPUT_TARGET for r in [result_small['throughput'], result_medium['throughput']]]):\n", "        print(f\"  â€¢ Ingestion throughput below target\")\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 9: Cleanup\n"]
    },
    {
      "cell_type": "code",
      "execute_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["result_small['lsh'].clear()\n", "result_medium['lsh'].clear()\n", "result_large['lsh'].clear()\n", "\n", "print(\"âœ“ Benchmark complete - Redis cleaned up\")\n"]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}